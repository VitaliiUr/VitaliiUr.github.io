<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>VU | Bayesian linear regression and Metropolis-Hastings sampler</title>
<meta name="description" content="Personal Website.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.xyz/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üßê</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/blog/2021/linreg/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

  <script src="/assets/js/theme.js"></script>
  <!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    
  </head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "Bayesian linear regression and Metropolis-Hastings sampler",
      "description": "on Python",
      "published": "June 11, 2021",
      "authors": [
        
        {
          "author": "Vitalii Urbanevych",
          "authorURL": "",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body class=" sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <!-- VU -->
       <span class="font-weight-bold">Vitalii</span>  Urbanevych
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/CV/">
                CV
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="post distill">

      <d-title>
        <h1>Bayesian linear regression and Metropolis-Hastings sampler</h1>
        <p>on Python</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <div class="row mt-3" style="margin-bottom: 18px">
    <div class="col-sm mt-3 mt-md-0" align="center">
        <img class="img-fluid rounded z-depth-1" src="/assets/img/linreg/linear_regression.png " />
    </div>
</div>

<h2 id="model-definition">Model definition</h2>

<p><strong>NOTE:</strong>
Theoretical considerations are mainly taken from <d-cite key="bishop_pattern"></d-cite>.</p>

<p>Let‚Äôs consider a set of observations $\mathbf{x}$ together
with observations of the target variable $t$.</p>

<p>Assume that our target variable $t$ is given by a deterministic function $y(\mathbf{x}, \mathbf{w})$ with additive Gaussian noise:</p>

\[\begin{equation}
  t = y(\mathbf{x}, \mathbf{w}) + \epsilon,
  \label{model}
\end{equation}\]

<p>where $\epsilon$ defines a random Gaussian noise with a zero mean and precision equal $\beta$ (inverse variance).</p>

<p>The function $y(\mathbf{x}, \mathbf{w})$ defines our model as <strong>x</strong> is a data and <strong>w</strong>
is a parameters vector.</p>

<p>\begin{equation}
  y(\mathbf{x}, \mathbf{w}) = \mathbf{w}^T \mathbf{\phi}(\mathbf{x}),
  \label{basfunc}
\end{equation}</p>

<p>where $\mathbf{\phi}(\mathbf{x})$ is a set of basis functions with a dummy zero component
$\phi_0(\mathbf{x}) = 1$. These functions can belong to different families(for example Gaussian
or sigmoid functions), but in all that cases the model will be called <em>linear regression</em>
as it will be linear for $\mathbf{w}$. The simplest choice of basis functions where $y(\mathbf{x}, \mathbf{w}) = \mathbf{w}^T \mathbf{x}$.</p>

<p>So we can write</p>

\[\begin{equation}
  p (t \mid \mathbf{x}, \mathbf{w}, \beta) = N(t \mid \mathbf{w}^T \mathbf{\phi}(\mathbf{x}), \beta^{-1})
  \label{norm}
\end{equation}\]

<p>Assuming that data points are independent and generated from (\ref{norm}), we can write a likelihood function as a function from $\mathbf{w}$:</p>

<p>\begin{equation}
  p (\mathbf{t} \mid \mathbf{X}, \mathbf{w}, \beta) = \prod_{i=1}^N N(t_i \mid \mathbf{w}^T \mathbf{\phi}(\mathbf{x}_i), \beta^{-1}),
  \label{likelihood}
\end{equation}</p>

<p>where $\mathbf{X}$ is a vector of inputs $(\mathbf{x}_1, \mathbf{x}_2,‚Ä¶,\mathbf{x}_N)$ and $\mathbf{t}$ is a vector of target points
$\mathbf{t} = (t_1, t_2,‚Ä¶, t_N)$ and $\beta$ - a precision parameter.</p>

<p>Maximizing a log of likelihood (\ref{likelihood}) with respect to $\mathbf{w}$ one can easili obtain an expression for the fitting parameters $\mathbf{w}$ (so-called <code class="language-plaintext highlighter-rouge">Normal equation</code>)</p>

<p>\begin{equation}
  \mathbf{w}_{ML} = \left(\Phi^T\Phi\right)^{-1}\Phi^T\mathbf{t},
  \label{maximized1}
\end{equation}</p>

<p>where $\Phi _{ij} = \phi_j(\mathbf{x}_i)$</p>

<p>Here we can generate some data and check the model. Let‚Äôs generate 
a set of 10 observations from the linear function $t = -3x+2+N(0,1/\beta)$ with $\beta = 1.0$.
We wind the fitting parameters using <code class="language-plaintext highlighter-rouge">normal equation</code>.</p>

<details>
<summary>Click to see the code</summary>
  <d-code block="" language="python">
    import numpy as np
    import scipy
    import matplotlib.pyplot as plt

    np.random.seed(42)
    N=10
    w_orig = np.array([-3.0, 2.0]) # true values of the parameters
    x = np.random.uniform(-1, 1, N)
    beta = 1.0 # noize precision (inverse variance)
    PHI =np.vstack((x,np.ones(x.shape[0]))).T # x with additional columns with ones (\Phi matrix)
    noize = np.random.normal(0.0, scale=1.0/np.sqrt(beta), size=N)
    t = PHI@w_orig + noize # target

    w_ml = (inv(PHI.T.dot(PHI))).dot(PHI.T.dot(t)) # normal equation

    plt.scatter(x, t, marker="x", label="data")
    plt.plot(x,PHI.dot(w_ml), c="tab:green", label="normal equation")
    plt.plot(x,PHI.dot(w_orig), c="tab:red", label="true line")
    plt.xlabel("x")
    plt.ylabel("t")
    plt.legend()
    plt.show()
  </d-code>
</details>

<div class="row mt-3" style="margin-bottom: 18px">
    <div class="col-sm mt-3 mt-md-0" align="center">
        <img class="img-fluid rounded z-depth-1" src="/assets/img/linreg/fited-ml.png" />
    </div>
</div>

<d-code block="" language="python">
    &gt;&gt;&gt; print("Original parameters:", *w_orig)
    &gt;&gt;&gt; print("Normal equation parameters:", *np.round(w_ml,2))
    Original parameters: -3.0 2.0
    Normal equation parameters: -2.25 1.42
  </d-code>

<p>The difference with true values is $\sim 25\%$ but this is the maximum result which can be obtained from such a dataset, so whese values could be regarded as a reference for other models.</p>

<p>One can also apply an iterative approach,
e.g. <code class="language-plaintext highlighter-rouge">Gradient descent</code>. In this case, we set some zero approximation for the parameters and gradually come closer to true values. The demonstration of such a procedure is on the figure below.</p>

<details>
<summary>Click to see the code</summary>
  <d-code block="" language="python">
    w0 = np.array([0.0, 0.0])[np.newaxis,:] # first guess
    eta = 0.15 # learning rate

    xc = np.linspace(-4,0.1, 100)
    yc = np.linspace(-0.1, 4, 100)

    X, Y = np.meshgrid(xc, yc)
    www = np.moveaxis(np.stack((X,Y)), 0, -1)
    Z = np.sum((t - www.dot(PHI.T))**2, axis=-1)

    for _ in range(50):
        w0 = np.vstack((w0, w0[-1] + eta*(t-w0[-1].dot(PHI.T)).dot(PHI)))
        
    plt.figure(figsize=(8,6))
    plt.contourf(X, Y, Z,50, cmap='pink')
    plt.scatter(*w_orig, facecolors='none', edgecolors='r', label="true value")
    plt.scatter(w0[1:,0], w0[1:,1], s=20)
    plt.plot(w0[:,0], w0[:,1],"--")
    plt.scatter(*w0[0], label="start", marker='x', color='red', s=80)
    plt.scatter(*w_ml, facecolors='none', c='lime',s=60, label="normal equation value")
    plt.xlabel("$w_0$")
    plt.ylabel("$w_1$")
    plt.legend()
    plt.savefig("../img/linreg/descent.png", dpi=500)
    plt.show()
  </d-code>

</details>

<div class="row mt-3" style="margin-bottom: 18px">
    <div class="col-sm mt-3 mt-md-0" align="center">
        <img class="img-fluid rounded z-depth-1" src="/assets/img/linreg/descent.png" />
    </div>
</div>
<div class="caption">
    Gradient descent
</div>

<hr />

<h2 id="bayesian-approach-ba">Bayesian approach (BA)</h2>

<div class="row mt-3" style="margin-bottom: 18px">
    <div class="col-sm mt-3 mt-md-0" align="center">
        <img class="img-fluid rounded z-depth-1" src="/assets/img/homo-bayesians.jpg" />
    </div>
</div>

<p>According to BA, we treat regression parameters not as unknown constants,
but as random variables. Therefore we shall define subjective information about
the set of parameters $\mathbf{w}$ as a prior distribution.
There are lots of possibilities but the  simplest is to 
set this distribution to be a Gaussian, 
as it is conjugate with respect to the likelihood
function (\ref{likelihood}).</p>

<p>\begin{equation}
  p(\mathbf{w}) = N(\mathbf{w} \mid \mathbf{m_0}, \mathbf{S_0}),
  \label{prior_w}
\end{equation}
where $\mathbf{m_0}$ and $\mathbf{S_0}$ are arbitrary defined as a first approximation.</p>

<p>With such a simple prior one can get the corresponding posterior distribution analytically (more details in <d-cite key="bishop_pattern"></d-cite>)</p>

\[\begin{equation}
  p(\mathbf{w} \mid \mathbf{t}, \mathbf{X}, \beta) = N(\mathbf{w} \mid \mathbf{m_1}, \mathbf{S_1}),
  \label{posterior_w}
\end{equation}\]

<p>whith well-defined parameters</p>

\[\begin{eqnarray}
  \mathbf{m}_1 &amp;=&amp; \mathbf{S}_1\left(\mathbf{S}_0^{-1}\mathbf{m_0} + \beta \mathbf{\Phi}^T\mathbf{t}\right)
  \label{m1}\\
  \mathbf{S}_1^{-1} &amp;=&amp; \mathbf{S}_0^{-1} + \beta \mathbf{\Phi}^T\mathbf{\Phi}.
  \label{S1}
\end{eqnarray}\]

<p>According to the Bayes equation, the (\ref{posterior_w}) is nothing else but normalized product of (\ref{likelihood}) and (\ref{prior_w}). $\mathbf{m}_1$ is a vector of means  and $\mathbf{S}_1$ is a covariance matrix for $\mathbf{w}$.</p>

<p>Choosing as a first approximation $\mathbf{m_0}=(0,0)$ and \(\mathbf{S_0} = \big(\begin{smallmatrix} 10 &amp; 0\\ 0 &amp; 10 \end{smallmatrix}\big)\), I obtained next results:</p>

<details>
  <summary>Click to see the code</summary>
    <d-code block="" language="python">
    def likelihood(w, t, PHI, beta):
        if t.shape:
            return np.prod(scipy.stats.norm.pdf(t,loc=w.dot(PHI.T),\
                           scale=1.0/np.sqrt(beta)), axis=-1)
        else:
            return scipy.stats.norm.pdf(t,loc=w.dot(PHI.T),\
                                        scale=1.0/np.sqrt(beta))
     
    def m1_S1(m0, S0, y, beta, PHI):
        if y.shape:
            S1 = np.linalg.inv(np.linalg.inv(S0) + beta*PHI.T@PHI)
            m1 = S1.dot(np.linalg.inv(S0).dot(m0) + beta*PHI.T.dot(y))
        else:
            S1 = np.linalg.inv(np.linalg.inv(S0) +\
                 beta*PHI[np.newaxis].T@PHI[np.newaxis])
            m1 = S1.dot(np.linalg.inv(S0).dot(m0) + beta*PHI.T*y)
        return m1, S1

    m0 = np.array([0,0])
    S0 = np.array([[10,0],[0,10]])
    w0 = np.linspace(-5,5,100)
    w1 = np.linspace(-5,5,100)
    w=np.array([[[val0,val1] for val0 in w0] for val1 in w1])
    pr = scipy.stats.multivariate_normal.pdf(w,mean=m0, cov=S0)

    lk = likelihood(w, t, PHI, beta)
    m1, S1 = m1_S1(m0, S0, t, beta, PHI)
    pr1 = scipy.stats.multivariate_normal.pdf(w,mean=m1, cov=S1)

    plt.figure(figsize=(15,5))
    plt.subplot(131)
    plt.contourf(w0, w1, pr, cmap='RdYlGn_r')
    plt.scatter(*w_orig, marker="+", s=100, c="b", label="true value")
    plt.scatter(*w_ml, marker='x', c='black', s=50)
    plt.xlabel("$w_0$")
    plt.ylabel("$w_1$")
    plt.title("prior")
    plt.subplot(132)
    plt.contourf(w0,w1,pr1, cmap='RdYlGn_r')
    plt.scatter(*w_orig, marker='x', c='b', s=50, label="True")
    plt.scatter(*w_ml, marker='x', c='black', s=50, label="Normal equation")
    plt.legend()
    plt.title("posterior")
    plt.xlabel("$w_0$")
    plt.ylabel("$w_1$")
    plt.subplot(133)
    plt.scatter(x, t, marker='x', label='data')
    plt.plot(x,PHI.dot(m1), c="tab:green", label="label="bayesian mean")
    plt.plot(x,PHI.dot(w_orig), c="tab:red", label="true line")
    plt.legend()
    plt.xlabel("x")
    plt.ylabel("t")
    plt.xlim([-1,1])
    plt.ylim([-1,5])
    plt.tight_layout()
    plt.show()
  </d-code>

</details>

<div class="row mt-3" style="margin-bottom: 18px">
    <div class="col-sm mt-3 mt-md-0" align="center">
        <img class="img-fluid rounded z-depth-1" src="/assets/img/linreg/bayesian_fit.png" />
    </div>
</div>
<div class="caption">
    Prior(left), posterior(middle) and fitted line with parameters as a mean of the posterior.
</div>

<p>with</p>

\[\begin{eqnarray}
  \mathbf{m_1} &amp;=&amp; (-2.18,1.4)
  \label{m1_res}\\
  \mathbf{S_1}  &amp;=&amp; \big(\begin{smallmatrix}  0.27 &amp; -0.01\\ -0.01 &amp; 0.1 \end{smallmatrix}\big).
  \label{S1_res}
\end{eqnarray}\]

<p>The values of $\mathbf{m_1}$ are close to the values of $\mathbf{w}$ obtained within the frequentists‚Äô approach (FA), but shifted towards $\mathbf{m_0}$ values.</p>

<hr />

<h2 id="metropolis-hastings-sampler-for-the-posterior-distribution"><a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm">Metropolis-Hastings sampler</a> for the posterior distribution</h2>

<p>Although it was easy enough to get the posterior distribution in the previous example, it is not always the case. Metropolis-Hastings method allows to sample values from complex probability functions. As posterior is proportional to the product of the likelihood and prior, it is enough to sample from such a product to find all the neccessary distribution parameters.</p>

<p>The first step is to test the Metropolis-Hastings sampler on the same normal prior. We know the analytical result so we can evaluate the precision of such model.</p>

<p>The starting point for the sampling is chosen to be $(0,0)$, the step $\delta = 0.5$ and the number of iterations is $N = 10^4$. The sampling result is below.</p>

<details>
  <summary>Click to see the code</summary>
    <d-code block="" language="python">
    def posterior(w, PHI, y, beta, m0, S0):
      return likelihood(w, y, PHI, beta)*prior(w, m0, S0)

    samples = np.array([0.0, 0.0])[np.newaxis]

    # for the background colormap
    w_0 = np.linspace(-6,1,100)
    w_1 = np.linspace(-1, 5 ,100)
    w=np.array([[[val0,val1] for val0 in w_0] for val1 in w_1])
    pr = prior(w, m0, S0)
    lk = likelihood(w, data_y, data_PHI, beta) 

    # sampling
    while samples.shape[0] &lt; N:
    candidate = samples[-1]+stats.uniform.rvs(loc=-delta, scale=2*delta, size=2)
    alpha = min(1, posterior(candidate, data_PHI, data_y, beta, m0, S0)\
                   /posterior(samples[-1], data_PHI, data_y, beta, m0, S0))
    u = stats.uniform.rvs()
    if u &lt; alpha:
        samples = np.vstack((samples, candidate))

    # animation
    from celluloid import Camera

    fig = plt.figure(figsize=(8,8))
    camera = Camera(fig)
    plt.title("Posterior", fontsize=18)
    plt.xlabel("$w_0$")
    plt.ylabel("$w_1$")
    plt.xlim([-6,1])
    plt.plot(samples[:1,0], samples[:1,1],".-",
             markersize=5, alpha=0.8,
             color="deeppink", zorder=1,
             label="samples")
    plt.scatter(samples[0,0], samples[0,1],
                marker="x", s=100, c="white",
                zorder=2, label="starting point")
    plt.legend(framealpha=0.3, facecolor='bisque')
    for i in np.unique(np.geomspace(1,samples.shape[0],
                       dtype=int, num=30)):
        plt.contourf(w_0,w_1,pr*lk, cmap='RdYlGn_r')
        plt.plot(samples[:i,0], samples[:i,1],".-",
                 markersize=5, alpha=0.8,
                 color="deeppink", zorder=1,
                 label="samples")
        plt.scatter(samples[0,0], samples[0,1],
                    marker="x", s=100, c="white",
                    zorder=2, label="starting point")
        camera.snap()
    animation = camera.animate()
    animation.save(filename, fps=4, dpi=300)
  </d-code>
</details>

<div class="row mt-3" style="margin-bottom: 18px">
    <div class="col-sm mt-3 mt-md-0" align="center">
        <img class="img-fluid rounded z-depth-1" src="/assets/img/linreg/anim.gif" />
    </div>
</div>
<div class="caption">
    Metropolis-Hastings sampling
</div>

<p>Now we can find mean values of the samples and their covariance matrix.</p>

<d-code block="" language="python">
  &gt;&gt;&gt; print("Mean:", np.round(np.mean(samples, axis=0),2))
  &gt;&gt;&gt; print("Covariance:\n", np.round(np.cov(samples.T),2))
    Mean: [-2.2  1.4]
    Covariance:
          [[ 0.27 -0.02]
          [-0.02  0.1 ]]
</d-code>

<p>These values are quite close to analytical (\ref{m1_res}) and (\ref{S1_res}). So this method sufficiently approximates posterior probability disribution.</p>

<hr />

<p>We can observe, that the numerical approach works, so we can define the prior in a more complicated way and numerically estimate the regression parameters.</p>

<p>I define the prior distribution as a combiantion of two Normal distributions and apply it for the same dataset.</p>

<d-code block="" language="python">
def prior(w):
  return 0.5*scipy.stats.multivariate_normal.pdf(w, mean=[-2, 0],
                                                 cov=[[2,0],[0, 0.9]])+\
      0.5*scipy.stats.multivariate_normal.pdf(w, mean=[-1.5, 3],
                                              cov=[[0.9,-0.0],[0.0, 1.8]])
</d-code>

<details>
  <summary>Click to see the code</summary>
    <d-code block="" language="python">
      w_0 = np.linspace(-5,5,100)
      w_1 = np.linspace(-5,5,100)
      w=np.array([[[val0,val1] for val0 in w_0] for val1 in w_1])
      
      lk = likelihood(w, t, PHI, beta)
      pr = prior(w)

      plt.figure(figsize=(10,5))
      plt.subplot(121)
      plt.contourf(w_0,w_1,pr, cmap='RdYlGn_r', levels=15)
      plt.scatter(*w_orig, marker='x', c='b', s=50, label="True")
      plt.scatter(*w_ml, marker='x', c='black', s=50, label="Normal equation")
      plt.title("prior")
      plt.xlabel(r"$w_0$")
      plt.ylabel(r"$w_1$")
      plt.subplot(122)
      plt.contourf(w_0,w_1,pr*lk, cmap='RdYlGn_r')
      plt.scatter(*w_orig, marker='x', c='b', s=50, label="True")
      plt.scatter(*w_ml, marker='x', c='black', s=50, label="Normal equation")
      plt.legend()
      plt.title(r"prior$\times$likelihood")
      plt.xlabel(r"$w_0$")
      plt.ylabel(r"$w_1$")
      plt.show()
  </d-code>
</details>

<div class="row mt-3" style="margin-bottom: 18px">
    <div class="col-sm mt-3 mt-md-0" align="center">
        <img class="img-fluid rounded z-depth-1" src="/assets/img/linreg/compl_prior.png" />
    </div>
</div>

<p>Now I can generate samples using the same numerical algorythm.</p>

<div class="row mt-3" style="margin-bottom: 18px">
    <div class="col-sm mt-3 mt-md-0" align="center">
        <img class="img-fluid rounded z-depth-1" src="/assets/img/linreg/anim2.gif" />
    </div>
</div>

<d-code block="" language="python">
  &gt;&gt;&gt; print("Mean:", np.mean(samples, axis=0))
  &gt;&gt;&gt; print("Covariance:\n", np.cov(samples.T))
    Mean: [-1.83022425  1.64481683]
    Covariance:
    [[ 0.62099147 -0.04302909]
    [-0.04302909  0.4241194 ]]
</d-code>

<p>These values are still quite close to analytical (\ref{m1_res}) and (\ref{S1_res}). The prior I have chosen is very unrealistic, so, in fact, it only complicates the procedure. Nevertheless, the sampling method works well and makes possible to retrieve distribution parameters.</p>

<hr />

<p>In BA we regard parameters as random variables, so we are not satisfied with only point estimations of the target variable $t$. We would like to estimate a credible interval for our prediction as well. For the Gaussian conjugated prior it is also possible to obtain analytical solution. Still, numerical methods can be used for the predictive distribution.</p>

<p>I will demonstrate it in the next post.</p>

<!-- ## Predictive distribution

At the moment we have obtained an estimation of the regression parameters within the frequentists' and bayesian approach both analytically and numerically. We can build a fitted line and make a predictions for new points.

But in BA we regard parameters as random variables, so we are not satisfied with only point estimations. Also we would like to find a credible interval for our prediction. In <d-cite key="bishop_pattern"></d-cite> the predictiv -->

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-article>

        
        <div id="disqus_thread"></div>
        <script type="text/javascript">
          var disqus_shortname = 'vitaliiur-github-io';
          var disqus_identifier = '/blog/2021/linreg';
          var disqus_title = "Bayesian linear regression and Metropolis-Hastings sampler";
          (function () {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
          })();
          </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
          Disqus.</a></noscript>
          
        </d-article>
          
    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    <!-- &copy; Copyright 2021 Vitalii  Urbanevych. -->
    &copy; 2021 Vitalii Urbanevych. All rights reserved.
    
    
    
    
  </div>
</footer>



  </body>

  <d-bibliography src="/assets/bibliography/2021-06-05_linreg.bib">
  </d-bibliography>

</html>
